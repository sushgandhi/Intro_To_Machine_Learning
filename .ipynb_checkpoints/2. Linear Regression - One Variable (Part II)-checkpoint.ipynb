{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in Data Science finding the \"Best Model\" for a certain situation is \"minimizing the error of the model\".\n",
    "\n",
    "Other way to saying this is...<br>\n",
    "We are solving an optmisation problem.\n",
    "\n",
    "\n",
    "The idea for solving these optmisation problems is called <b>Gradient Descent</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent id not only used for minimising the cost function, rather to minimise any function needed.\n",
    "\n",
    "Let's start by taking an example of minimising some arbitary function and minimising it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Process Outline :\n",
    "\n",
    "- Have some function $J(\\theta_0,\\theta_1)$\n",
    "- We want min ($\\theta_0,\\theta_1) J(\\theta_0,\\theta_1)$\n",
    "- Start with some value of $\\theta_0,\\theta_1$\n",
    "- keep changing $\\theta_0,\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/grad_desc_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^\n",
    "Here, if we visualize of plorring of $\\theta_0,\\theta_1$ against Function J.\n",
    "\n",
    "we can start with any value of $\\theta_0,\\theta_1$ and see J.\n",
    "\n",
    "Then change values of $\\theta_0,\\theta_1$ and calculate the J again.\n",
    "\n",
    "If the value of J is less than previous calculated one. Then we are moving in the right direction, if it is greater, we are going in wrong direction.\n",
    "\n",
    "We can visualize is being on top of a hill and then going downhill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another property of Gradient Descent is, that we can have local minima as well.\n",
    "e.g - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/grad_desc_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta_j = \\theta_j - \\alpha \\frac {\\partial J( \\theta_0,\\theta_1)}{\\partial \\theta_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $ \\alpha$ is the learning rate.\n",
    "\n",
    "Or in out above example how fast(big step) we will go downhill.\n",
    "\n",
    "If $\\alpha$ is very large, it corresponds to very aggresive gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac {\\partial J( \\theta_0,\\theta_1)}{\\partial \\theta_j}$\n",
    "\n",
    "^^ This is the derivative of cost function with the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Parallel to calculating the $\\theta$ we need to update it as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/grad_desc_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and understand the dereivate if, it was for a single variable function.\n",
    "\n",
    "J($\\theta$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/grad_desc_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^ this is the representation of the function $J(\\theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we say the derivative..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/grad_desc_5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^ Derivative is nothing but drawing a tangent at that point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now checking the slope of the line.\n",
    "\n",
    "This derivative has a positive slope.\n",
    "\n",
    "now if derivative is +ve.\n",
    "\n",
    "$\\theta_1 = \\theta_1 + \\alpha (+ve Number) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^ Here theta is gonna decrease, now we are moving closer to the minimum over their."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derivative can be negetive as well, that means, increase in \\theta, but again  funciton would move towards to minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see $\\alpha$\n",
    "\n",
    "if $\\alpha$ is too small, gradaient descent will be too slow.\n",
    "\n",
    "But if $\\alpha$ is too large, gradient descent can overshoot the minumum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Scenario </b>\n",
    "\n",
    "If $\\theta$ is already at local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/grad_desc_6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here slope of the tangent will be 0.\n",
    "\n",
    "When the slope 0 derivative will be 0.\n",
    "\n",
    "so $\\theta_1$ will remain unchanges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the reason why gradient descent can converge to local minimum, even with learning rate fixed.\n",
    "\n",
    "As we approach the local minimum, the gradient descent will automatically take small steps (as slope value will be less). So, no need to decrease $\\alpha$ over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of $J(\\theta_0,\\theta_1)$ \n",
    "\n",
    "we will have\n",
    "\n",
    "derivative of $ \\frac{1}{2m}\\sum_{i=1}^m (h_0(x^i)-y^i)^2$\n",
    "\n",
    "$ \\frac {\\partial}{\\partial \\theta_j}\\frac{1}{2m}\\sum_{i=1}^m (\\theta_0 + \\theta_1 x^i-y^i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating for j = 0 and j = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\theta_0 j = 0 : \\frac {\\partial}{\\partial \\theta_0} J (\\theta_0,\\theta_1) = \\frac {1} {m} \\sum_{i=1}^m ( h_\\theta (x^i) - y^i)\n",
    "$$\n",
    "$$\n",
    "\\theta_1 j = 1 : \\frac {\\partial}{\\partial \\theta_0} J (\\theta_0,\\theta_1) = \\frac {1} {m} \\sum_{i=1}^m ( h_\\theta (x^i) - y^i).x^i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"img/grad_desc_8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function for gradient descent is always a convex function....\n",
    "so local minima should be same as global minima\n",
    "\n",
    "here's the visualization - \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/grad_Desc_7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^ This is called <b>batch gradient descent...</b>\n",
    "\n",
    "as each step of gradient descent uses all the training examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
